{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Asteroid Light Curve Examples - Part 1\n",
    "\n",
    "This notebook contains examples deep learning techniques applied to the asteroid light curve data from http://alcdef.org.\n",
    "\n",
    "\n",
    "# Objectives\n",
    "- Understand when a convolutional neural network (CNN) might be applicable.\n",
    "- See how to apply a 1D-CNN to time-series data.\n",
    "- See how to build a more complex model that takes both time-series and categorical inputs.\n",
    "\n",
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the ALCDEF_ALL dataset downloaded from http://alcdef.org\n",
    "# Download the full archive as a .zip file. Extract its contents to this\n",
    "# directory. It should be ~14K .txt files.\n",
    "data_dir = 'data/ALCDEF_ALL'\n",
    "\n",
    "# Discard any light curves with fewer than this many samples\n",
    "min_samples = 100\n",
    "\n",
    "# Resample light curves to common number of samples\n",
    "nb_samples = 100\n",
    "\n",
    "# Discard any light curve that isn't among the nb_classes most common objects\n",
    "nb_classes = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy.signal import resample\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling1D, Input\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ml4ssa_utils import visualize_embedding, load_alcdef_data, plot_alcdef_examples, normalize_features, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Dataset\n",
    "\n",
    "Load data from Astroid Lightcurve Photometry Database (http://alcdef.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_alcdef_data(\n",
    "    data_dir=data_dir,\n",
    "    min_samples=min_samples,\n",
    "    resample_to=nb_samples,\n",
    "    reduce_to_top=nb_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gather a list of the object names we'll be working with\n",
    "names = list(set([item['OBJECTNAME'] for item in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_alcdef_examples(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([ item['DATA_RESAMPLED'][:,1] for item in data ])\n",
    "y = np.array([ names.index(item['OBJECTNAME']) for item in data ])\n",
    "\n",
    "# Reserve 20% of the data for testing\n",
    "# Startify the data split so that the train and test sets have the same class distribution\n",
    "X_train, X_test, y_train, y_test, data_train, data_test = train_test_split(X, y, data, test_size=0.20, stratify=y)\n",
    "\n",
    "print('Generated train and test sets with the following sizes:')\n",
    "print('Train X (features) {}, y (targets) {}'.format(X_train.shape, y_train.shape))\n",
    "print('Test  X (features) {}, y (targets) {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Class Distributions to Understand Performance of Random Classifier\n",
    "\n",
    "It's always helpful to understand how well a random classifier should perform. This sets a worst case baseline. If you're doing better than this performance, you know at least something is working. If your classifier is performing worse than random, something is broken. If it's performing at the same level as random, it's either broken or you have a very hard problem (at least with your current size and distribution of training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.sum(to_categorical(y_test), axis=0)\n",
    "class_proportions = class_counts / np.sum(class_counts)\n",
    "max_proportion = np.max(class_proportions)\n",
    "random_performance = 1./nb_classes\n",
    "\n",
    "print('Random Performance: {:.3f}'.format(1./nb_classes))\n",
    "print('Mode Collapse Performance: {:.3f}'.format(max_proportion))\n",
    "print('-'*65)\n",
    "for name, proportion in zip(names, class_proportions):\n",
    "    print('{:15} {:.3f} {}'.format(name, proportion, 'largest' if proportion == max_proportion else ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try an MLP (multi-layer perceptron) similiar to the TLE Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='acc'\n",
    "\n",
    "nb_classes = len(names)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100,activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=[metric]\n",
    ")\n",
    "\n",
    "# keras is complaining that I need to evaluate the model before printing a summary\n",
    "# model.predict(np.zeros((16,9)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, 'Untrained MLP on Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to pass to model\n",
    "repeats = 100\n",
    "# The repeats value here is used to artifically increase the size of our training set.\n",
    "# This forces keras to treat <repeats> passes through the training set as a single epoch and we\n",
    "# get to avoid a huge number of progress bars and short-term variance in metrics.\n",
    "train_features = normalize_features(X_train.repeat(repeats, axis=0))\n",
    "train_targets = to_categorical(y_train.repeat(repeats, axis=0))\n",
    "test_features = normalize_features(X_test)\n",
    "test_targets = to_categorical(y_test)\n",
    "\n",
    "# Fit model to data\n",
    "model.fit(\n",
    "    train_features, train_targets,\n",
    "    validation_data=(test_features, test_targets),\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    callbacks=[EarlyStopping(patience=3, monitor='val_loss')],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, 'Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_train, y_train, 'Train Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "\n",
    "Now that we have two baselines (random performance and the MLP we used for TLE data), let's look at improving our performance with a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='acc'\n",
    "\n",
    "nb_classes = len(names)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters=16, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(units=nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[metric]\n",
    ")\n",
    "\n",
    "# keras is complaining that I need to evaluate the model before printing a summary\n",
    "model.predict(np.zeros((16,100,1)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to pass to model\n",
    "repeats = 100\n",
    "# The repeats value here is used to artifically increase the size of our training set.\n",
    "# This forces keras to treat <repeats> passes through the training set as a single epoch and we\n",
    "# get to avoid a huge number of progress bars and short-term variance in metrics.\n",
    "train_features = normalize_features(X_train.repeat(repeats, axis=0))\n",
    "train_targets = to_categorical(y_train.repeat(repeats, axis=0))\n",
    "test_features = normalize_features(X_test)\n",
    "test_targets = to_categorical(y_test)\n",
    "\n",
    "# The convolutional layers will expect a \"channels\" dimension at the end.\n",
    "train_features = np.expand_dims(train_features, axis=-1)\n",
    "test_features = np.expand_dims(test_features, axis=-1)\n",
    "\n",
    "model.fit(\n",
    "    train_features, train_targets,\n",
    "    validation_data=(test_features, test_targets),\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    callbacks=[EarlyStopping(patience=5, monitor='val_loss')],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Curve Embedding Based on Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we extract the intermediate features/activations from the layer named penultimate\n",
    "layer_name = 'penultimate'\n",
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_penultimate_test = intermediate_layer_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embedding(X_penultimate_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Modal Input\n",
    "\n",
    "The above models use features from single modality (sampled light curves). In real world problems, we often have multiple data types that will be relevant to our problem. For exampoe, we typically at least have metadata associated with sampled data.\n",
    "\n",
    "The convolutional layers were motivated by the assumption that our sampled data was translationally invariant. As we have no reason to believe this should be the case for our metadata (it's not even clear what that would mean), we'll need to think about how best to incorporate additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_metadata_vector(item):\n",
    "    '''Generate a metadata vector of the form <one-hot-encoded filter value> | <phase>.\n",
    "    \n",
    "    The data appears to have 3 different filter codes and a single phase value so the metadata vector\n",
    "    will be of length 4.\n",
    "    '''\n",
    "    v = np.zeros(4)\n",
    "    filter_codes = ['V', 'R', 'C']\n",
    "    filter_ndx = filter_codes.index(item['FILTER'])\n",
    "    v[filter_ndx] = 1\n",
    "    v[-1] = float(item['PHASE']) / 60. # 60 was chosen as it was the largest value observed in a chunk of the data\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='acc'\n",
    "\n",
    "nb_classes = len(names)\n",
    "\n",
    "nb_metadata_inputs = 4\n",
    "\n",
    "cnn_input = Input(shape=(nb_samples,1), name='cnn_input')\n",
    "x = Conv1D(filters=64, kernel_size=5, activation='relu')(cnn_input)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Conv1D(filters=32, kernel_size=5, activation='relu')(x)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Conv1D(filters=16, kernel_size=5, activation='relu')(x)\n",
    "cnn_output = GlobalAveragePooling1D()(x)\n",
    "\n",
    "metadata_input = Input(shape=(nb_metadata_inputs,), name='metadata_input')\n",
    "x = Dense(units=10, activation='relu')(metadata_input)\n",
    "metadata_output = Dense(units=10, activation='relu')(x)\n",
    "\n",
    "merged = keras.layers.concatenate([cnn_output, metadata_output])\n",
    "\n",
    "final_output = Dense(units=nb_classes, activation='softmax')(merged)\n",
    "\n",
    "model = Model([cnn_input, metadata_input], final_output)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=[metric]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to pass to model\n",
    "repeats = 100\n",
    "# The repeats value here is used to artifically increase the size of our training set.\n",
    "# This forces keras to treat <repeats> passes through the training set as a single epoch and we\n",
    "# get to avoid a huge number of progress bars and short-term variance in metrics.\n",
    "train_features = normalize_features(X_train.repeat(repeats, axis=0))\n",
    "train_targets = to_categorical(y_train.repeat(repeats, axis=0))\n",
    "test_features = normalize_features(X_test)\n",
    "test_targets = to_categorical(y_test)\n",
    "\n",
    "# The convolutional layers will expect a \"channels\" dimension at the end.\n",
    "train_features = np.expand_dims(train_features, axis=-1)\n",
    "test_features = np.expand_dims(test_features, axis=-1)\n",
    "\n",
    "# TODO: Fix this to use actual metadata features\n",
    "train_metadata_features = np.stack([ generate_metadata_vector(item) for item in data_train ]).repeat(repeats, axis=0)\n",
    "test_metadata_features = np.stack([ generate_metadata_vector(item) for item in data_test ])\n",
    "\n",
    "model.fit(\n",
    "    [train_features, train_metadata_features], train_targets,\n",
    "    validation_data=([test_features, test_metadata_features], test_targets),\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    callbacks=[EarlyStopping(patience=5, monitor='val_loss')],\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
